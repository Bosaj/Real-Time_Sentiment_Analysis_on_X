{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark==3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'socketserver' has no attribute 'UnixStreamServer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m spark = SparkSession.builder \\\n\u001b[32m      4\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mSentimentAnalysis\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      5\u001b[39m     .master(\u001b[33m\"\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.executor.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m12g\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      7\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m12g\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      8\u001b[39m     .getOrCreate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ROG FLOW\\anaconda3\\Lib\\site-packages\\pyspark\\__init__.py:71\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InheritableThread, inheritable_thread_target\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstoragelevel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StorageLevel\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maccumulators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accumulator, AccumulatorParam\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserializers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MarshalSerializer, CPickleSerializer\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtaskcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaskContext, BarrierTaskContext, BarrierTaskInfo\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ROG FLOW\\anaconda3\\Lib\\site-packages\\pyspark\\accumulators.py:324\u001b[39m\n\u001b[32m    320\u001b[39m         \u001b[38;5;28msuper\u001b[39m().shutdown()\n\u001b[32m    321\u001b[39m         \u001b[38;5;28mself\u001b[39m.server_close()\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAccumulatorUnixServer\u001b[39;00m(\u001b[43msocketserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUnixStreamServer\u001b[49m):\n\u001b[32m    325\u001b[39m     server_shutdown = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mself\u001b[39m, socket_path: \u001b[38;5;28mstr\u001b[39m, RequestHandlerClass: Type[socketserver.BaseRequestHandler]\n\u001b[32m    329\u001b[39m     ):\n",
      "\u001b[31mAttributeError\u001b[39m: module 'socketserver' has no attribute 'UnixStreamServer'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, StringType, IntegerType\n\u001b[32m      3\u001b[39m schema = StructType([\n\u001b[32m      4\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mTweetID\u001b[39m\u001b[33m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      5\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mEntity\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      6\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      7\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mContent\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m ])\n\u001b[32m     10\u001b[39m df = spark.read.csv(\u001b[33m\"\u001b[39m\u001b[33m./Spark/X_training.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mFalse\u001b[39;00m, schema=schema)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"TweetID\", IntegerType(), True),\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Sentiment\", StringType(), True),\n",
    "    StructField(\"Content\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"./Spark/X_training.csv\", header=False, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(df[\"Content\"].isNotNull())\n",
    "df = df.fillna({\"Content\": \"default_value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|TweetID|     Entity|Sentiment|             Content|               words|            filtered|         rawFeatures|            features|\n",
      "+-------+-----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   2401|Borderlands| Positive|im getting on bor...|[im, getting, on,...|[im, getting, bor...|(262144,[31015,92...|(262144,[31015,92...|\n",
      "|   2401|Borderlands| Positive|I am coming to th...|[i, am, coming, t...|[coming, borders,...|(262144,[12409,14...|(262144,[12409,14...|\n",
      "|   2401|Borderlands| Positive|im getting on bor...|[im, getting, on,...|[im, getting, bor...|(262144,[31015,68...|(262144,[31015,68...|\n",
      "|   2401|Borderlands| Positive|im coming on bord...|[im, coming, on, ...|[im, coming, bord...|(262144,[12409,31...|(262144,[12409,31...|\n",
      "|   2401|Borderlands| Positive|im getting on bor...|[im, getting, on,...|[im, getting, bor...|(262144,[12524,31...|(262144,[12524,31...|\n",
      "|   2401|Borderlands| Positive|im getting into b...|[im, getting, int...|[im, getting, bor...|(262144,[31015,68...|(262144,[31015,68...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|[so, i, spent, a,...|[spent, hours, ma...|(262144,[640,2182...|(262144,[640,2182...|\n",
      "|   2402|Borderlands| Positive|So I spent a coup...|[so, i, spent, a,...|[spent, couple, h...|(262144,[21823,26...|(262144,[21823,26...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|[so, i, spent, a,...|[spent, hours, so...|(262144,[21823,45...|(262144,[21823,45...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|[so, i, spent, a,...|[spent, hours, ma...|(262144,[640,2182...|(262144,[640,2182...|\n",
      "|   2402|Borderlands| Positive|2010 So I spent a...|[2010, so, i, spe...|[2010, spent, hou...|(262144,[640,2182...|(262144,[640,2182...|\n",
      "|   2402|Borderlands| Positive|                 was|               [was]|                  []|      (262144,[],[])|      (262144,[],[])|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,51...|(262144,[19937,51...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,51...|(262144,[19937,51...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,51...|(262144,[19937,51...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Vita...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,60...|(262144,[19937,60...|\n",
      "|   2403|Borderlands|  Neutral|Live Rock - Hard ...|[live, rock, -, h...|[live, rock, -, h...|(262144,[2437,988...|(262144,[2437,988...|\n",
      "|   2403|Borderlands|  Neutral|I-Hard like me, R...|[i-hard, like, me...|[i-hard, like, me...|(262144,[27971,63...|(262144,[27971,63...|\n",
      "|   2404|Borderlands| Positive|that was the firs...|[that, was, the, ...|[first, borderlan...|(262144,[16793,11...|(262144,[16793,11...|\n",
      "|   2404|Borderlands| Positive|this was the firs...|[this, was, the, ...|[first, borderlan...|(262144,[110357,1...|(262144,[110357,1...|\n",
      "+-------+-----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)  \n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    return text\n",
    "\n",
    "preprocess_text_udf = spark.udf.register(\"preprocess_text\", preprocess_text)\n",
    "df = df.withColumn(\"Content\", preprocess_text_udf(\"Content\"))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Content\", outputCol=\"words\")\n",
    "df_words = tokenizer.transform(df)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df_filtered_words = remover.transform(df_words)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(df_filtered_words)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
    "df_final = indexer.fit(rescaledData).transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.866391\n",
      "Best model parameters:\n",
      "regParam: 0.01\n",
      "elasticNetParam: 0.0\n",
      "maxIter: 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.01])  \n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \n",
    "             .addGrid(lr.maxIter, [10, 50, 100]) \n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds = 3)  \n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "predictions = cvModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Logistic Regression Accuracy: %f\" % accuracy)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "print(\"Best model parameters:\")\n",
    "print(\"regParam:\", bestModel._java_obj.getRegParam())\n",
    "print(\"elasticNetParam:\", bestModel._java_obj.getElasticNetParam())\n",
    "print(\"maxIter:\", bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Accuracy: 0.825080\n",
      "Best model parameters:\n",
      "Smoothing parameter: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label')\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0.0, 1.0, 2.0])  \n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=nb,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  \n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Naive Bayes Classifier Accuracy: %f\" % accuracy)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "print(\"Best model parameters:\")\n",
    "print(\"Smoothing parameter:\", bestModel._java_obj.getSmoothing())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
